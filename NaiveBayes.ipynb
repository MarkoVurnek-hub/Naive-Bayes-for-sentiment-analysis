{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Sentiment analysis').getOrCreate();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was collected using the Twitter API for use in the paper:\n",
    "Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(12).\n",
    "Link: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('data/Sentiment140.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "|  0|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "|  0|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "|  0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "|  0|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "|  0|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "|  0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "|  0|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "|  0|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
      "|  0|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "|  0|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  4|1990082099|Mon Jun 01 03:49:...|NO_QUERY|      Poptastic|@detoxcute aww, s...|\n",
      "|  4|1990082104|Mon Jun 01 03:49:...|NO_QUERY|    gary_walker|@samtaylor256 Wel...|\n",
      "|  4|1990082129|Mon Jun 01 03:49:...|NO_QUERY|     Monica2112|@TimothyH2O night...|\n",
      "|  4|1990082164|Mon Jun 01 03:49:...|NO_QUERY|   mizzlizwhizz|@kingsunshine It ...|\n",
      "|  4|1990082170|Mon Jun 01 03:49:...|NO_QUERY| ronnyvengeance|scoreee.hot johnn...|\n",
      "|  4|1990082195|Mon Jun 01 03:49:...|NO_QUERY|    blogbrevity|@jayecane Thank y...|\n",
      "|  4|1990082215|Mon Jun 01 03:49:...|NO_QUERY|   H0TCOMMODITY|#musicmonday love...|\n",
      "|  4|1990082286|Mon Jun 01 03:49:...|NO_QUERY|   linzintha804|Spongebob helps m...|\n",
      "|  4|1990082309|Mon Jun 01 03:49:...|NO_QUERY|        pennnny|@greyhoundstooth ...|\n",
      "|  4|1990082383|Mon Jun 01 03:49:...|NO_QUERY|  MareeAnderson|Goodnight everybo...|\n",
      "|  4|1990082403|Mon Jun 01 03:49:...|NO_QUERY| WParenthetical|@essjay that's ok...|\n",
      "|  4|1990082464|Mon Jun 01 03:49:...|NO_QUERY|          JulzM|We has just put '...|\n",
      "|  4|1990082477|Mon Jun 01 03:49:...|NO_QUERY|     101ofawolf|@everythingabili,...|\n",
      "|  4|1990082494|Mon Jun 01 03:49:...|NO_QUERY|        emilyyh|&quot;Mothers, ot...|\n",
      "|  4|1990082557|Mon Jun 01 03:49:...|NO_QUERY|     MRamos1292|is almost done wi...|\n",
      "|  4|1990082599|Mon Jun 01 03:49:...|NO_QUERY|      EricJ_art|BleedingCool.com ...|\n",
      "|  4|1990082631|Mon Jun 01 03:49:...|NO_QUERY|       arovilla|@TomFelton hey to...|\n",
      "|  4|1990082704|Mon Jun 01 03:49:...|NO_QUERY|    MelissaWOOF|@turnitgrey Thats...|\n",
      "|  4|1990082729|Mon Jun 01 03:49:...|NO_QUERY|KatyCountryGirl|Last night- Count...|\n",
      "|  4|1990082743|Mon Jun 01 03:49:...|NO_QUERY|      Josie_Goh|just let out an a...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView('test')\n",
    "result = spark.sql('SELECT * FROM test ORDER BY _c0 DESC').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_columns = data.select(['_c0', '_c5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|_c0|                 _c5|\n",
      "+---+--------------------+\n",
      "|  0|@switchfoot http:...|\n",
      "|  0|is upset that he ...|\n",
      "|  0|@Kenichan I dived...|\n",
      "|  0|my whole body fee...|\n",
      "|  0|@nationwideclass ...|\n",
      "|  0|@Kwesidei not the...|\n",
      "|  0|         Need a hug |\n",
      "|  0|@LOLTrish hey  lo...|\n",
      "|  0|@Tatiana_K nope t...|\n",
      "|  0|@twittera que me ...|\n",
      "|  0|spring break in p...|\n",
      "|  0|I just re-pierced...|\n",
      "|  0|@caregiving I cou...|\n",
      "|  0|@octolinz16 It it...|\n",
      "|  0|@smarrison i woul...|\n",
      "|  0|@iamjazzyfizzle I...|\n",
      "|  0|Hollis' death sce...|\n",
      "|  0|about to file taxes |\n",
      "|  0|@LettyA ahh ive a...|\n",
      "|  0|@FakerPattyPattz ...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns = filter_columns.withColumnRenamed('_c0', 'class').withColumnRenamed('_c5', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|    0|@switchfoot http:...|\n",
      "|    0|is upset that he ...|\n",
      "|    0|@Kenichan I dived...|\n",
      "|    0|my whole body fee...|\n",
      "|    0|@nationwideclass ...|\n",
      "|    0|@Kwesidei not the...|\n",
      "|    0|         Need a hug |\n",
      "|    0|@LOLTrish hey  lo...|\n",
      "|    0|@Tatiana_K nope t...|\n",
      "|    0|@twittera que me ...|\n",
      "|    0|spring break in p...|\n",
      "|    0|I just re-pierced...|\n",
      "|    0|@caregiving I cou...|\n",
      "|    0|@octolinz16 It it...|\n",
      "|    0|@smarrison i woul...|\n",
      "|    0|@iamjazzyfizzle I...|\n",
      "|    0|Hollis' death sce...|\n",
      "|    0|about to file taxes |\n",
      "|    0|@LettyA ahh ive a...|\n",
      "|    0|@FakerPattyPattz ...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rename_columns.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT class)|\n",
      "+---------------------+\n",
      "|                    2|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rename_columns.agg(countDistinct('class')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|class| count|\n",
      "+-----+------+\n",
      "|    0|800000|\n",
      "|    4|800000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rename_columns.groupBy('class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "remove_character = rename_columns.withColumn('text', regexp_replace('text', '@', ''))\n",
    "change_class_negative = remove_character.withColumn('class', regexp_replace('class', '0', 'negative'))\n",
    "change_class_positive = change_class_negative.withColumn('class', regexp_replace('class', '4', 'positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|class   |text                                                                                                                |\n",
      "+--------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|negative|switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |\n",
      "|negative|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!     |\n",
      "|negative|Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |\n",
      "|negative|my whole body feels itchy and like its on fire                                                                      |\n",
      "|negative|nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|negative|Kwesidei not the whole crew                                                                                         |\n",
      "|negative|Need a hug                                                                                                          |\n",
      "|negative|LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|negative|Tatiana_K nope they didn't have it                                                                                  |\n",
      "|negative|twittera que me muera ?                                                                                             |\n",
      "|negative|spring break in plain city... it's snowing                                                                          |\n",
      "|negative|I just re-pierced my ears                                                                                           |\n",
      "|negative|caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|negative|octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |\n",
      "|negative|smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|negative|iamjazzyfizzle I wish I got to watch it with you!! I miss you and iamlilnicki  how was the premiere?!               |\n",
      "|negative|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                       |\n",
      "|negative|about to file taxes                                                                                                 |\n",
      "|negative|LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|negative|FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+--------+--------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "change_class_positive.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|class   |text                                                                                                                                     |\n",
      "+--------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|positive|detoxcute aww, sorry! i'll see if i can include you in a future post                                                                     |\n",
      "|positive|samtaylor256 Well they certainly are cheerful                                                                                            |\n",
      "|positive|TimothyH2O night, love.                                                                                                                  |\n",
      "|positive|kingsunshine It is beautifully hot today, isn't it. Was up till late last night and then again early - think I need a siesta             |\n",
      "|positive|scoreee.hot johnny depp picture for June! (Y) i love my johnny depp calender                                                             |\n",
      "|positive|jayecane Thank you for musical kiss                                                                                                      |\n",
      "|positive|#musicmonday love like this - natasha ft. sean kingston.. old song but i love it                                                         |\n",
      "|positive|Spongebob helps me not hate the people hatin on me.                                                                                      |\n",
      "|positive|greyhoundstooth i.... killed it. by dropg it on e floor. haha! thanks for wishg me luck! i nd it!                                        |\n",
      "|positive|Goodnight everybody! Thanks for making 1st June a day to remember                                                                        |\n",
      "|positive|essjay that's okay  life takes over sometimes!                                                                                           |\n",
      "|positive|We has just put 'Malcolm' in to watch...love that my kid loves good old Oz flicks.                                                       |\n",
      "|positive|everythingabili, timhasting's tagwalk project reminded me of your Engagement Engine + it's good, creative ways to work data.             |\n",
      "|positive|&quot;Mothers, others and Jonas Brothers&quot; wow, that would've been a great name for an album !  &lt;3 haha.                          |\n",
      "|positive|is almost done with this school shit.                                                                                                    |\n",
      "|positive|BleedingCool.com is LIVE! Everyone retweet! Retweet like your lives depend on it! (via richjohnston) Good luck, with the new venture,man |\n",
      "|positive|TomFelton hey tom! I'm divy from indonesian. What are you up to now?                                                                     |\n",
      "|positive|turnitgrey Thats what it told me :0 imma still go  ahah                                                                                  |\n",
      "|positive|Last night- Country fest This morning-Dead tired but I got me a couple of hot guys last night  Well worth it                             |\n",
      "|positive|just let out an accidental fart~ ooOOps~                                                                                                 |\n",
      "+--------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "change_class_positive.createOrReplaceTempView('test')\n",
    "result = spark.sql('SELECT * FROM test ORDER BY class DESC').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (Tokenizer, StopWordsRemover, IDF, CountVectorizer, StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|token_text                                                                                                                                  |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[switchfoot, http://twitpic.com/2y1zl, -, awww,, that's, a, bummer., , you, shoulda, got, david, carr, of, third, day, to, do, it., ;d]     |\n",
      "|[is, upset, that, he, can't, update, his, facebook, by, texting, it..., and, might, cry, as, a, result, , school, today, also., blah!]      |\n",
      "|[kenichan, i, dived, many, times, for, the, ball., managed, to, save, 50%, , the, rest, go, out, of, bounds]                                |\n",
      "|[my, whole, body, feels, itchy, and, like, its, on, fire]                                                                                   |\n",
      "|[nationwideclass, no,, it's, not, behaving, at, all., i'm, mad., why, am, i, here?, because, i, can't, see, you, all, over, there.]         |\n",
      "|[kwesidei, not, the, whole, crew]                                                                                                           |\n",
      "|[need, a, hug]                                                                                                                              |\n",
      "|[loltrish, hey, , long, time, no, see!, yes.., rains, a, bit, ,only, a, bit, , lol, ,, i'm, fine, thanks, ,, how's, you, ?]                 |\n",
      "|[tatiana_k, nope, they, didn't, have, it]                                                                                                   |\n",
      "|[twittera, que, me, muera, ?]                                                                                                               |\n",
      "|[spring, break, in, plain, city..., it's, snowing]                                                                                          |\n",
      "|[i, just, re-pierced, my, ears]                                                                                                             |\n",
      "|[caregiving, i, couldn't, bear, to, watch, it., , and, i, thought, the, ua, loss, was, embarrassing, ., ., ., ., .]                         |\n",
      "|[octolinz16, it, it, counts,, idk, why, i, did, either., you, never, talk, to, me, anymore]                                                 |\n",
      "|[smarrison, i, would've, been, the, first,, but, i, didn't, have, a, gun., , , , not, really, though,, zac, snyder's, just, a, doucheclown.]|\n",
      "|[iamjazzyfizzle, i, wish, i, got, to, watch, it, with, you!!, i, miss, you, and, iamlilnicki, , how, was, the, premiere?!]                  |\n",
      "|[hollis', death, scene, will, hurt, me, severely, to, watch, on, film, , wry, is, directors, cut, not, out, now?]                           |\n",
      "|[about, to, file, taxes]                                                                                                                    |\n",
      "|[lettya, ahh, ive, always, wanted, to, see, rent, , love, the, soundtrack!!]                                                                |\n",
      "|[fakerpattypattz, oh, dear., were, you, drinking, out, of, the, forgotten, table, drinks?]                                                  |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
    "tokenized = tokenizer.transform(change_class_positive)\n",
    "tokenized.select('token_text').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "For tasks like text classification, where the text is to be classified into different categories, stopwords are removed or excluded from the given text so that more focus can be given to those words which define the meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_remover = StopWordsRemover(inputCol='token_text', outputCol='stop_token', locale='en_US')\n",
    "stop_remover.getStopWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer\n",
    "\n",
    "Provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(inputCol='stop_token', outputCol='c_vect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse document frequency\n",
    "\n",
    "It’s a score which the machine keeps where it is evaluates the words used in a sentence and measures it’s usage compared to words used in the entire document. In other words, it’s a score to highlight each word’s relevance in the entire document. It’s calculated as -\n",
    "IDF =Log[(Number of documents) / (Number of documents containing the word)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='c_vect', outputCol='tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_numeric = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+\n",
      "|   class|                text|length|\n",
      "+--------+--------------------+------+\n",
      "|negative|switchfoot http:/...|   114|\n",
      "|negative|is upset that he ...|   111|\n",
      "|negative|Kenichan I dived ...|    88|\n",
      "|negative|my whole body fee...|    47|\n",
      "|negative|nationwideclass n...|   110|\n",
      "|negative|Kwesidei not the ...|    28|\n",
      "|negative|         Need a hug |    11|\n",
      "|negative|LOLTrish hey  lon...|    98|\n",
      "|negative|Tatiana_K nope th...|    35|\n",
      "|negative|twittera que me m...|    24|\n",
      "|negative|spring break in p...|    43|\n",
      "|negative|I just re-pierced...|    26|\n",
      "|negative|caregiving I coul...|    93|\n",
      "|negative|octolinz16 It it ...|    76|\n",
      "|negative|smarrison i would...|   116|\n",
      "|negative|iamjazzyfizzle I ...|   101|\n",
      "|negative|Hollis' death sce...|    93|\n",
      "|negative|about to file taxes |    20|\n",
      "|negative|LettyA ahh ive al...|    63|\n",
      "|negative|FakerPattyPattz O...|    78|\n",
      "+--------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "data = change_class_positive.withColumn('length', length(change_class_positive['text']))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleanup = VectorAssembler(inputCols=['tf_idf'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "Set of data processing elements connected in series, where the output of one element is the input of the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "data_pipeline = Pipeline(stages=[to_numeric, tokenizer, stop_remover, count_vect, idf, data_cleanup])\n",
    "\n",
    "# Run stages in pipeline and train model\n",
    "pipelineFit = data_pipeline.fit(training_data)\n",
    "train_data_df = pipelineFit.transform(training_data)\n",
    "#Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "test_data_df = pipelineFit.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem.\n",
    "\n",
    "*P(A|B) = (P(B|A) * P (A)) / P (B)*\n",
    "\n",
    "Using Bayes theorem, we can find the probability of A happening, given that B has occurred. Here, B is the evidence and A is the hypothesis. The assumption made here is that the predictors/features are independent. That is presence of one particular feature does not affect the other. Hence it is called naive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "naive_bayes = NaiveBayes()\n",
    "nb_model = naive_bayes.fit(train_data_df)\n",
    "prediction = nb_model.transform(test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "accuracy_evaluation = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7451486771089897\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = accuracy_evaluation.evaluate(prediction)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+\n",
      "|   class|                text|length|\n",
      "+--------+--------------------+------+\n",
      "|negative|          .. Omga...|   132|\n",
      "|negative|     &lt;- but mu...|    49|\n",
      "|negative|     &lt;--------...|    60|\n",
      "|negative|     I dont like ...|    42|\n",
      "|negative|     I'll get on ...|    31|\n",
      "|negative|     what the fuc...|    30|\n",
      "|negative|    &lt;-------- ...|    52|\n",
      "|negative|     ...lonely night|    19|\n",
      "|negative|    I'll be worki...|    71|\n",
      "|negative|    Not feeling i...|    95|\n",
      "|negative|   I am going to ...|    79|\n",
      "|negative|   I'm thinking o...|    94|\n",
      "|negative|   bad day.....da...|    67|\n",
      "|negative|   i think my bf ...|    43|\n",
      "|negative|   it relly sux t...|    97|\n",
      "|negative|   kinda but not ...|    35|\n",
      "|negative|      no shopping   |    17|\n",
      "|negative|  -- that's an EP...|    29|\n",
      "|negative|  Another expensi...|    44|\n",
      "|negative|  Exercise 2 buil...|    94|\n",
      "+--------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   class|                text|length|label|          token_text|          stop_token|              c_vect|              tf_idf|            features|\n",
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|negative|          .. Omga...|   132|  1.0|[, , , , , , , , ...|[, , , , , , , , ...|(262144,[0,1,13,2...|(262144,[0,1,13,2...|(262144,[0,1,13,2...|\n",
      "|negative|     &lt;- but mu...|    49|  1.0|[, , , , , &lt;-,...|[, , , , , &lt;-,...|(262144,[0,198,31...|(262144,[0,198,31...|(262144,[0,198,31...|\n",
      "|negative|     &lt;--------...|    60|  1.0|[, , , , , &lt;--...|[, , , , , &lt;--...|(262144,[0,199,39...|(262144,[0,199,39...|(262144,[0,199,39...|\n",
      "|negative|     I dont like ...|    42|  1.0|[, , , , , i, don...|[, , , , , dont, ...|(262144,[0,3,64,1...|(262144,[0,3,64,1...|(262144,[0,3,64,1...|\n",
      "|negative|     I'll get on ...|    31|  1.0|[, , , , , i'll, ...|[, , , , , get, r...|(262144,[0,1,50,2...|(262144,[0,1,50,2...|(262144,[0,1,50,2...|\n",
      "|negative|     what the fuc...|    30|  1.0|[, , , , , what, ...|[, , , , , fucccc...|  (262144,[0],[5.0])|(262144,[0],[5.42...|(262144,[0],[5.42...|\n",
      "|negative|    &lt;-------- ...|    52|  1.0|[, , , , &lt;----...|[, , , , &lt;----...|(262144,[0,34,50,...|(262144,[0,34,50,...|(262144,[0,34,50,...|\n",
      "|negative|     ...lonely night|    19|  1.0|[, , , , ...lonel...|[, , , , ...lonel...|(262144,[0,36],[4...|(262144,[0,36],[4...|(262144,[0,36],[4...|\n",
      "|negative|    I'll be worki...|    71|  1.0|[, , , , i'll, be...|[, , , , working,...|(262144,[0,45,79,...|(262144,[0,45,79,...|(262144,[0,45,79,...|\n",
      "|negative|    Not feeling i...|    95|  1.0|[, , , , not, fee...|[, , , , feeling,...|(262144,[0,25,40,...|(262144,[0,25,40,...|(262144,[0,25,40,...|\n",
      "|negative|   I am going to ...|    79|  1.0|[, , , i, am, goi...|[, , , going, she...|(262144,[0,3,5,6,...|(262144,[0,3,5,6,...|(262144,[0,3,5,6,...|\n",
      "|negative|   I'm thinking o...|    94|  1.0|[, , , i'm, think...|[, , , thinking, ...|(262144,[0,22,162...|(262144,[0,22,162...|(262144,[0,22,162...|\n",
      "|negative|   bad day.....da...|    67|  1.0|[, , , bad, day.....|[, , , bad, day.....|(262144,[0,47,383...|(262144,[0,47,383...|(262144,[0,47,383...|\n",
      "|negative|   i think my bf ...|    43|  1.0|[, , , i, think, ...|[, , , think, bf,...|(262144,[0,24,151...|(262144,[0,24,151...|(262144,[0,24,151...|\n",
      "|negative|   it relly sux t...|    97|  1.0|[, , , it, relly,...|[, , , relly, sux...|(262144,[0,22,28,...|(262144,[0,22,28,...|(262144,[0,22,28,...|\n",
      "|negative|   kinda but not ...|    35|  1.0|[, , , kinda, but...|[, , , kinda, rea...|(262144,[0,295,19...|(262144,[0,295,19...|(262144,[0,295,19...|\n",
      "|negative|      no shopping   |    17|  1.0|[, , , no, shopping]|    [, , , shopping]|(262144,[0,425],[...|(262144,[0,425],[...|(262144,[0,425],[...|\n",
      "|negative|  -- that's an EP...|    29|  1.0|[, , --, that's, ...|[, , --, epic, sa...|(262144,[0,48,658...|(262144,[0,48,658...|(262144,[0,48,658...|\n",
      "|negative|  Another expensi...|    44|  1.0|[, , another, exp...|[, , another, exp...|(262144,[0,84,118...|(262144,[0,84,118...|(262144,[0,84,118...|\n",
      "|negative|  Exercise 2 buil...|    94|  1.0|[, , exercise, 2,...|[, , exercise, 2,...|(262144,[0,28,114...|(262144,[0,28,114...|(262144,[0,28,114...|\n",
      "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+\n",
      "|   class|                text|         probability|prediction|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "|negative|          .. Omga...|[8.05564448203288...|       1.0|\n",
      "|negative|     &lt;- but mu...|[0.02508221644595...|       1.0|\n",
      "|negative|     &lt;--------...|[2.56098374982433...|       1.0|\n",
      "|negative|     I dont like ...|[1.07348442974731...|       1.0|\n",
      "|negative|     I'll get on ...|[0.00270180391215...|       1.0|\n",
      "|negative|     what the fuc...|[0.39599571803864...|       1.0|\n",
      "|negative|    &lt;-------- ...|[2.04432018357065...|       1.0|\n",
      "|negative|     ...lonely night|[0.59111375775565...|       0.0|\n",
      "|negative|    I'll be worki...|[5.92108641114615...|       1.0|\n",
      "|negative|    Not feeling i...|[4.46987256991297...|       1.0|\n",
      "|negative|   I am going to ...|[2.21351474870513...|       1.0|\n",
      "|negative|   I'm thinking o...|[0.99999998171228...|       0.0|\n",
      "|negative|   bad day.....da...|[8.84465970150609...|       1.0|\n",
      "|negative|   i think my bf ...|[7.27670897436831...|       1.0|\n",
      "|negative|   it relly sux t...|[7.43317441138027...|       1.0|\n",
      "|negative|   kinda but not ...|[5.05961115381214...|       1.0|\n",
      "|negative|      no shopping   |[0.93184902860507...|       0.0|\n",
      "|negative|  -- that's an EP...|[1.94009779240250...|       1.0|\n",
      "|negative|  Another expensi...|[6.76247058110815...|       1.0|\n",
      "|negative|  Exercise 2 buil...|[1.12922493089646...|       1.0|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(['class', 'text', 'probability', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------------------------+----------+\n",
      "|text       |probability                               |prediction|\n",
      "+-----------+------------------------------------------+----------+\n",
      "|Great news!|[0.9999818660887572,1.8133911242880134E-5]|0.0       |\n",
      "+-----------+------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "  \n",
    "data = {'text':['Great news!']} \n",
    "  \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "input_value = spark.createDataFrame(df)\n",
    "test_data_NEW = pipelineFit.transform(input_value)\n",
    "prediction = nb_model.transform(test_data_NEW)\n",
    "prediction.select(['text', 'probability', 'prediction']).show(truncate=False)\n",
    "# 1 indicates a negative polarity and 0 a positive polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
